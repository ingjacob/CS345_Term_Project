{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181e235d-06c1-4b0d-b714-ef59ef8410bb",
   "metadata": {},
   "source": [
    "# Music Genera Classification With the GTZAN dataset\n",
    "CS345 Fall 2024 Project   \n",
    "Wade McCaulley  \n",
    "Jacob Ingraham  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3218ee2f",
   "metadata": {},
   "source": [
    "The topic we wish to address with this project is Music Genre Classification. Music Genre Classification has applications ranging from improved music recommendations to audio tagging for large audio libraries, which can help create more personalized music choices on platforms like Spotify. Since machine learning models specialize in extracting information from highly dimensional data, we believe that music data presents a strong opportunity. Since a song’s genre is one of the most defining aspects of a song, this classification is very well suited for machine learning. A genre is determined by many aspects such as the tempo, rhythm, instrumentation, and overall tone. \n",
    "\n",
    "This topic is interesting because music is an expression of emotion and a work of art. Many people prefer certain music genres because of how the songs impact their emotions. The idea of transforming songs into a collection of data points and accurately classifying them into genres is intriguing. This topic will also be challenging because audio data is very complex. Our models will have to find patterns hidden within the noise and complexity.\n",
    "\n",
    "By comparing many different machine learning approaches we hope to learn more about this topic. The models we are comparing are K-Nearest Neighbor (KNN), Support Vector Machine (SVM), Random Forest, and Convolutional Neural Network (CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98ce2c5-e32c-4b8e-a55b-ecb3acc9460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All necessary imports for the following code\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc4e79-3859-4bcb-9e02-c04b647c2a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place these files in the appropriate directory: Data\n",
    "# Otherwise, change these filepaths\n",
    "# The files can be downloaded from https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification/data\n",
    "features_30_seconds_filepath = \"../Data/features_30_sec.csv\"\n",
    "features_3_seconds_filepath = \"../Data/features_3_sec.csv\"\n",
    "mel_spectrograms_filepath = \"../Data/images_original\"\n",
    "\n",
    "genres = [\"blues\", \"classical\" , \"country\", \"disco\", \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25018ed-539a-4884-8e36-e1c575e03ff1",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "The dataset consists of 10 genres, each containing 100 audio files, with each file having a duration of 30 seconds. This dataset is often referred to as \"The MNIST of sounds,\" drawing a comparison to the well-known MNIST database of handwritten digits frequently used in CS345.\n",
    "\n",
    "We did not manually extract features from the audio files. Instead, we utilized pre-processed data.\n",
    "\n",
    "Music Generes:\n",
    "* Blues\n",
    "* Classical\n",
    "* Country\n",
    "* Disco\n",
    "* Hiphop\n",
    "* Jazz\n",
    "* Metal\n",
    "* Pop\n",
    "* Reggae\n",
    "* Rock\n",
    "\n",
    "### Images\n",
    "\n",
    "Images are visual representations of each audio file in the form of Mel Spectrograms. A Mel Spectrogram converts audio signals into a visual format that emphasizes frequency and amplitude over the duration of the 30-second audio file. This representation aligns with human perception of sound, as the frequency axis is transformed into the Mel scale. The Mel scale captures frequencies in a way that reflects how humans perceive pitch, allowing the audio to be digitally represented as a waveform. This visual format can then be utilized for further analysis or processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66574b91",
   "metadata": {},
   "source": [
    "### Reading data\n",
    "The following cells read and format the data. They also split the data into train/validate/test sets to be used when choosing hyperparameters and evaluating results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906f4bc1-2f5d-4cfa-a837-2a3ce5d17674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the CSVs. Features are everything but the first col(filename), and the lables. The lables are the last column\n",
    "def loadCSVs(filepath):\n",
    "    data = pd.read_csv(filepath, dtype = object, delimiter = ',').values\n",
    "    X = data[:,2:-1]\n",
    "    y = data[:,-1:]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f664227-1b71-494d-abea-47dfa3d340a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will turn the genere lables into np.array of ints\n",
    "def lable_to_int(lables, genres):\n",
    "    lable_int = np.array(lables)\n",
    "    for i in range(len(genres)):\n",
    "        lable_int[lable_int==genres[i]]=i\n",
    "    return lable_int \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb525d5-d3cf-43a6-947b-974aff3e4a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the mel spectrograms into a np array of images. Each image is 288, 432 pixels, and each pixel is represented by four values\n",
    "def load_mel_spectrograms():\n",
    "    image_features = []\n",
    "    image_lables = []\n",
    "    for genre in genres:\n",
    "        print(\"Loading\", genre)\n",
    "        images_file_path = mel_spectrograms_filepath + \"/\" + genre\n",
    "        png_files = [f for f in os.listdir(images_file_path) if f.endswith('.png')]\n",
    "\n",
    "        for file in png_files:\n",
    "            file_path = images_file_path +\"/\"+ file\n",
    "            image = plt.imread(file_path)  # Load the image\n",
    "            image_features.append(image)\n",
    "            image_lables.append(genre)\n",
    "\n",
    "    return np.array(image_features), np.array(image_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea624ba4-f7df-44e0-920c-4bec63122ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from data file as string, then convert to a usable datatype (float)\n",
    "string_X_30sec, y_30sec = loadCSVs(features_30_seconds_filepath)\n",
    "X_30sec = string_X_30sec.astype(np.float64)\n",
    "string_X_3sec, y_3sec = loadCSVs(features_3_seconds_filepath)\n",
    "X_3sec = string_X_3sec.astype(np.float64)\n",
    "\n",
    "# reshape y to a 1d array\n",
    "y_30sec = y_30sec.ravel()\n",
    "y_3sec = y_3sec.ravel()\n",
    "\n",
    "print(X_30sec.shape, y_30sec.shape)\n",
    "print(X_3sec.shape, y_3sec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e833a67-c7af-4aac-b059-fafbcac845eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the spectrogram data\n",
    "X_images, y_images = load_mel_spectrograms()\n",
    "y_images = lable_to_int(y_images,genres)\n",
    "X_images.shape, y_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bc9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split spectrogram data into train/validate/test - fulltrain will be used in the last cell to train best classifiers\n",
    "#    with train and validate sets\n",
    "X_images_fulltrain, X_images_test, y_images_fulltrain, y_images_test = train_test_split(X_images, y_images, test_size=0.1, shuffle=True, random_state=7)\n",
    "X_images_train, X_images_val, y_images_train, y_images_val = train_test_split(X_images_fulltrain, y_images_fulltrain, test_size=0.2, shuffle=True, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7fab7f-0d3b-4598-98f7-503e461ecfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label vectors to usable datatype (int)\n",
    "y_30sec_int = lable_to_int(y_30sec, genres)\n",
    "y_3sec_int = lable_to_int(y_3sec, genres)\n",
    "y_images_int = lable_to_int(y_images, genres)\n",
    "y_30sec_int.shape, y_3sec_int.shape, y_images_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7eb730-7c49-46a8-ba43-90d96b6354e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normalized and standardized versions of data\n",
    "X_30sec_norm = (X_30sec-np.min(X_30sec, axis=0))/(np.max(X_30sec,axis=0)-np.min(X_30sec,axis=0))\n",
    "X_3sec_norm = (X_3sec-np.min(X_3sec, axis=0))/(np.max(X_3sec,axis=0)-np.min(X_3sec,axis=0))\n",
    "X_30sec_std = (X_30sec-np.mean(X_30sec, axis=0))/(np.std(X_30sec, axis=0))\n",
    "X_3sec_std = (X_3sec-np.mean(X_3sec, axis=0))/(np.std(X_3sec, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bdc6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check normalized and standardized data\n",
    "print(np.max(X_30sec_norm)==1,np.min(X_30sec_norm)==0)\n",
    "print(np.max(X_3sec_norm)==1,np.min(X_3sec_norm)==0)\n",
    "print(np.mean(X_30sec_std), np.std(X_30sec_std))\n",
    "print(np.mean(X_3sec_std), np.std(X_3sec_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30fd646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split csv data into train/validate/test - fulltrain will be used in the last cell to train best classifiers\n",
    "#    with train and validate sets\n",
    "X_30sec_fulltrain, X_30sec_test, y_30sec_fulltrain, y_30sec_test = train_test_split(X_30sec, y_30sec, test_size=0.1, shuffle=True, random_state=7)\n",
    "X_30sec_train, X_30sec_val, y_30sec_train, y_30sec_val = train_test_split(X_30sec_fulltrain, y_30sec_fulltrain, test_size=0.2, shuffle=True, random_state=7)\n",
    "\n",
    "X_30sec_norm_fulltrain, X_30sec_norm_test = train_test_split(X_30sec_norm, test_size=0.1, shuffle=True, random_state=7)\n",
    "X_30sec_norm_train, X_30sec_norm_val = train_test_split(X_30sec_norm_fulltrain, test_size=0.2, shuffle=True, random_state=7)\n",
    "\n",
    "X_30sec_std_fulltrain, X_30sec_std_test = train_test_split(X_30sec_std, test_size=0.1, shuffle=True, random_state=7)\n",
    "X_30sec_std_train, X_30sec_std_val = train_test_split(X_30sec_std_fulltrain, test_size=0.2, shuffle=True, random_state=7)\n",
    "\n",
    "X_3sec_fulltrain, X_3sec_test, y_3sec_fulltrain, y_3sec_test = train_test_split(X_3sec, y_3sec, test_size=0.1, shuffle=True, random_state=7)\n",
    "X_3sec_train, X_3sec_val, y_3sec_train, y_3sec_val = train_test_split(X_3sec_fulltrain, y_3sec_fulltrain, test_size=0.2, shuffle=True, random_state=7)\n",
    "\n",
    "X_3sec_norm_fulltrain, X_3sec_norm_test = train_test_split(X_3sec_norm, test_size=0.1, shuffle=True, random_state=7)\n",
    "X_3sec_norm_train, X_3sec_norm_val = train_test_split(X_3sec_norm_fulltrain, test_size=0.2, shuffle=True, random_state=7)\n",
    "\n",
    "X_3sec_std_fulltrain, X_3sec_std_test = train_test_split(X_3sec_std, test_size=0.1, shuffle=True, random_state=7)\n",
    "X_3sec_std_train, X_3sec_std_val = train_test_split(X_3sec_std_fulltrain, test_size=0.2, shuffle=True, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632557ab-66dc-457d-811e-af31a4b5ca60",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "For Model Selection we used stratified K-Fold cross-validation with shuffling to ensure that each fold is not composed of consecutive samples. This approach helps maintain approximately the same number of samples from each genre in each fold, preventing some classes from being underrepresented.\n",
    "\n",
    "We used accuracy_score as the metric for cross-validation. When accuracy_score is selected, it evaluates the model's performance based on its accuracy.\n",
    "\n",
    "To automate the search for the best hyperparameters, we utilized GridSearchCV. This step was extremely slow, as we aimed to set hyperparameters for each model in a consistent manner.\n",
    "\n",
    "The runtime for k-fold cross-validation is proportional to (O(n \\times k)), where (n) is the sample size and (k) is the number of folds. We selected 5 folds, which reduces computational complexity but increases runtime. There is a risk of overfitting if one split performs significantly better than others. Fewer folds may result in less reliable estimates.\n",
    "\n",
    "The results were outputted in a DataFrame for easy analysis. We chose the hyperparameters for the dataset with the best accuracy among all datasets.\n",
    "\n",
    "GridSearchCV and cross-validation are computationally intensive because they systematically test multiple parameter combinations and validate performance across data subsets. During this process, the model is trained multiple times—once for each fold of cross-validation for each parameter combination.\n",
    "\n",
    "A high-dimensional parameter grid increases the number of combinations exponentially. For example, if you have 5 hyperparameters A and 5 hyperparameters B, this requires 25 (5x5) different combinations. Adding a third hyperparameter C increases this to 125 (5x5x5) combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cb6aea-915a-4cf9-89b4-e8d8aa9d4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gridsearch function which will be used by knn, svm, random forest to choose hyperparameters\n",
    "def girdSearchClassifier(model, features, labels, paramgrid):\n",
    "    start_time = time.time()\n",
    "    cv = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "    classifier = GridSearchCV(model, paramgrid)\n",
    "    classifier.fit(features, labels)\n",
    "    accuracies = cross_val_score(classifier.best_estimator_, features, labels, cv=cv, \n",
    "                           scoring='accuracy')\n",
    "    accuracy = np.mean(accuracies)\n",
    "    run_time = time.time() - start_time\n",
    "\n",
    "\n",
    "    return classifier.best_estimator_, accuracy, run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fea6121-029d-4f37-8a02-a2325084fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_datasets_30sec = [\n",
    "    (X_30sec, y_30sec, \"Features 30 sec\"),\n",
    "    (X_30sec_norm, y_30sec, \"Features 30 sec norm\"),\n",
    "    (X_30sec_std, y_30sec, \"Features 30 sec std\")\n",
    "]\n",
    "\n",
    "features_datasets_3sec = [\n",
    "    (X_3sec, y_3sec, \"Features 3 sec\"),\n",
    "    (X_3sec_norm, y_3sec, \"Features 3 sec norm\"),\n",
    "    (X_3sec_std, y_3sec, \"Features 3 sec std\")\n",
    "]\n",
    "\n",
    "features_datasets = [\n",
    "    (X_30sec, y_30sec, \"Features 30 sec\"),\n",
    "    (X_30sec_norm, y_30sec, \"Features 30 sec norm\"),\n",
    "    (X_30sec_std, y_30sec, \"Features 30 sec std\"),\n",
    "    (X_3sec, y_3sec, \"Features 3 sec\"),\n",
    "    (X_3sec_norm, y_3sec, \"Features 3 sec norm\"),\n",
    "    (X_3sec_std, y_3sec, \"Features 3 sec std\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed679947-de17-4120-8bd7-e6297157328b",
   "metadata": {},
   "source": [
    "## Nearest Neighbor \n",
    "The Nearest Neighbor classifier operates by identifying the example in the training dataset whose features most closely match those of the data point that needs to be classified. It then assigns the label of this closest example to the new data point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be87950-99dc-4e5b-abf2-70666ef796b9",
   "metadata": {},
   "source": [
    "#### Nearest Neighbor Hyperparameters\n",
    "\n",
    "The number of neighbors, denoted as n_neighbors, is the most critical hyperparameter in the Nearest Neighbor classifier. It determines how many neighbors are considered when making a prediction.\n",
    "\n",
    "The performance of the nearest neighbor classifier can be enhanced by basing the classification on multiple neighbors. For this testing, we are using the k-Nearest Neighbors (k-NN) classifier, which compares the (k) nearest neighbors to make a decision.\n",
    "\n",
    "\n",
    "#### Nearest Neighbor Running Time\n",
    "The heares neighbor classifier has a running time of $O(N * d)$ where n is the number of training examples and d is the number of dimensions in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f09c880-b8d1-4437-98de-4988901bbb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for choosing KNN hyperparameters on various datasets\n",
    "def testKNN(features, labels, paramgrid, valFeatures, valLabels):\n",
    "\n",
    "    model = KNeighborsClassifier()\n",
    "    best_estimator, accuracy, run_time = girdSearchClassifier(model, features, labels, paramgrid)\n",
    "    y_pred = best_estimator.predict(valFeatures)\n",
    "    val_accuracy = np.mean(y_pred == valLabels)\n",
    "    \n",
    "    return best_estimator.get_params()['n_neighbors'], accuracy, val_accuracy, run_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5997508-ef77-4ea6-871b-620020e68738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid and choose hyperparameters for each dataset\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [1,2,4,8,16,32,64,128,256,512]\n",
    "}\n",
    "\n",
    "print(\"knn run csv 30sec\")\n",
    "knn_csv30sec_best_estimator, knn_csv30sec_accuracy, knn_csv30sec_val_accuracy, knn_csv30sec_time = testKNN(X_30sec_train, y_30sec_train, knn_param_grid, X_30sec_val, y_30sec_val)\n",
    "\n",
    "print(\"knn run csv 30sec norm\")\n",
    "knn_csv30sec_norm_best_estimator, knn_csv30sec_norm_accuracy, knn_csv30sec_norm_val_accuracy, knn_csv30sec_norm_time = testKNN(X_30sec_norm_train, y_30sec_train, knn_param_grid, X_30sec_norm_val, y_30sec_val)\n",
    "\n",
    "print(\"knn run csv 30sec std\")\n",
    "knn_csv30sec_std_best_estimator, knn_csv30sec_std_accuracy, knn_csv30sec_std_val_accuracy, knn_csv30sec_std_time = testKNN(X_30sec_std_train, y_30sec_train, knn_param_grid, X_30sec_std_val, y_30sec_val)\n",
    "\n",
    "print(\"knn run csv 3sec\")\n",
    "knn_csv3sec_best_estimator, knn_csv3sec_accuracy, knn_csv3sec_val_accuracy, knn_csv3sec_time = testKNN(X_3sec_train, y_3sec_train, knn_param_grid, X_3sec_val, y_3sec_val)\n",
    "\n",
    "print(\"knn run csv 3sec norm\")\n",
    "knn_csv3sec_norm_best_estimator, knn_csv3sec_norm_accuracy, knn_csv3sec_norm_val_accuracy, knn_csv3sec_norm_time = testKNN(X_3sec_norm_train, y_3sec_train, knn_param_grid, X_3sec_norm_val, y_3sec_val)\n",
    "\n",
    "print(\"knn run csv 3sec std\")\n",
    "knn_csv3sec_std_best_estimator, knn_csv3sec_std_accuracy, knn_csv3sec_std_val_accuracy, knn_csv3sec_std_time = testKNN(X_3sec_std_train, y_3sec_train, knn_param_grid, X_3sec_std_val, y_3sec_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a023741d-a77b-423d-9e50-21de1adcc9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display hyperparameter choice results\n",
    "knndf = pd.DataFrame({\n",
    "    \"Dataset\": [\"Features 30 sec\", \"Features 30 sec norm\", \"Features 30 sec std\", \"Features 3 sec\", \"Features 3 sec norm\", \"Features 3 sec std\"],\n",
    "    \"Best n_neighbors\": [\n",
    "        knn_csv30sec_best_estimator,\n",
    "        knn_csv30sec_norm_best_estimator,\n",
    "        knn_csv30sec_std_best_estimator,\n",
    "        knn_csv3sec_best_estimator,\n",
    "        knn_csv3sec_norm_best_estimator,\n",
    "        knn_csv3sec_std_best_estimator\n",
    "    ],\n",
    "    \"Accuracy\": [\n",
    "        knn_csv30sec_accuracy,\n",
    "        knn_csv30sec_norm_accuracy,\n",
    "        knn_csv30sec_std_accuracy,\n",
    "        knn_csv3sec_accuracy,\n",
    "        knn_csv3sec_norm_accuracy,\n",
    "        knn_csv3sec_std_accuracy\n",
    "    ],\n",
    "    \"Validation Set Accuracy\": [\n",
    "        knn_csv30sec_val_accuracy,\n",
    "        knn_csv30sec_norm_val_accuracy,\n",
    "        knn_csv30sec_std_val_accuracy,\n",
    "        knn_csv3sec_val_accuracy,\n",
    "        knn_csv3sec_norm_val_accuracy,\n",
    "        knn_csv3sec_std_val_accuracy\n",
    "    ],\n",
    "    \"Run Time\": [\n",
    "        knn_csv30sec_time,\n",
    "        knn_csv30sec_norm_time,\n",
    "        knn_csv30sec_std_time,\n",
    "        knn_csv3sec_time,\n",
    "        knn_csv3sec_norm_time,\n",
    "        knn_csv3sec_std_time\n",
    "    ]\n",
    "})\n",
    "knndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ac2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean accuracy and mean runtime for all runs\n",
    "knn_hp_avg_accuracy = knndf[\"Accuracy\"].mean()\n",
    "knn_avg_runtime = knndf[\"Run Time\"].mean()\n",
    "\n",
    "knn_n_hyperparameters = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9518009-d5ac-4a6e-afa6-b966672dfad0",
   "metadata": {},
   "source": [
    "The results indicate that using 1 neighbor in the k-Nearest Neighbors (k-NN) classifier yielded the best performance, achieving the highest accuracy. A smaller k can lead to a model that is sensitive to noise in the data and may overfit, while a larger k can smooth out the decision boundary too much, potentially missing important patterns which may result in under fitting. Since the validation set accuracy is even higher than that of the test set, it is safe to assume that the model is not overfitting. \n",
    "\n",
    "The performance of k-NN improved when it was run on normalized and standardized data. This may be due to the algorithm's reliance on distance calculations. Scaling ensures that all features contribute equally to these calculations, preventing any single feature from disproportionately influencing the results. This leads to more accurate identification of nearest neighbors and better overall model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11292423-e20e-40c5-9613-7e4eb213dfe9",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "The SVM or Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It is typically used for binary classification problems, where the goal is to separate data points into two distinct classes. In SVM, the algorithm finds the optimal hyperplane that separates the data points of different classes. In a two-dimensional space, this hyperplane is a line, while in higher dimensions, it becomes a plane or a hyperplane\n",
    "\n",
    "For this experiment we ran two different SVM models. The first was a svm with a linear kernal and tested for the optimal C paramater. We also tested the rbf kernel and looked for the optimal c and gamma parameters. \n",
    "\n",
    "- C Parameter: The C parameter is a regularization parameter that controls the trade-off between achieving a low training error and a low testing error. A large value of C aims to classify all training examples correctly, which might lead to overfitting. Conversely, a small value of C allows for a larger margin, potentially at the cost of some misclassifications, which may result in under fitting.\n",
    "- Gamma Parameter: In the RBF kernel, the gamma parameter defines the influence of a single training example. A low gamma value means a large influence (far-reaching), resulting in a smoother decision boundary. A high gamma value means a small influence (close), leading to a more complex decision boundary that can adapt to the training data but may result in over fitting.\n",
    "\n",
    "Linear Kernel: In a linear kernel the  decision boundary is a hyperplane in the feature space. This means it separates data points from different classes in a linear fashion. They are most effective in data that is seperated by a straight line in 2d data or a plan in higher dimensions. \n",
    "\n",
    "RBF Kernel: In an RBF Kernel the decision boundary allows for more complex decision boundaries. These can be curves, circles, or more intricate shape. The RBF Kernel is also known as a Gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ebc0d8-5604-4271-9680-6a6ae1551d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for choosing SVM hyperparameters on various datasets\n",
    "def testSVM(features, labels, paramgrid, valFeatures, valLabels):\n",
    "    X_standard_scaler = StandardScaler().fit(features)\n",
    "    features = X_standard_scaler.transform(features)\n",
    "    model = svm.SVC()\n",
    "    best_estimator, accuracy, run_time = girdSearchClassifier(model, features, labels, paramgrid)\n",
    "    y_pred = best_estimator.predict(valFeatures)\n",
    "    val_accuracy = np.mean(y_pred == valLabels)\n",
    "    \n",
    "    return best_estimator.get_params()['C'], best_estimator.get_params()['gamma'], accuracy, val_accuracy, run_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279672f4-a1bc-45f7-9cde-0edd01bdcf05",
   "metadata": {},
   "source": [
    "### Features 3 seconds to large a dataset. Quadratic run time for larger data sets. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fcae82-17d4-4ebd-bac3-f92ba48a0c18",
   "metadata": {},
   "source": [
    "The computational complexity of SVM is between $On^2$ and $O(n^3)$ for training. This became computational expensive for this experiment. When combined with the kfold cross vaildation and gridsearch cv this became extreemly slow for the 10,000 example dataset with 3 seconds songs. Becuase of this we only examined the 1000 example 30 second data set. Below is an example of the exponential increase in training time based on the number of examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2d1e87-5841-4a4c-84a3-b6e4324e22f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate training time for SVM using data subsets of varying size\n",
    "sizes = [100, 250, 500, 1000, 2500, 5000, 9000]\n",
    "\n",
    "linear_runtimes = []\n",
    "rbf_runtimes = []\n",
    "\n",
    "print(\"testing SVM run time on large datasets\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_3sec, y_3sec, test_size = 0.01, random_state = 1)\n",
    "\n",
    "X_standard_scaler = StandardScaler().fit(X_train)\n",
    "X_train = X_standard_scaler.transform(X_train)\n",
    "for s in sizes:\n",
    "    start_time = time.time()\n",
    "    classifier = svm.SVC(C = 10, kernel='linear')\n",
    "    classifier.fit(X_train[:s], y_train[:s])\n",
    "    run_time = time.time() - start_time\n",
    "    linear_runtimes.append(run_time)\n",
    "\n",
    "for s in sizes:\n",
    "    start_time = time.time()\n",
    "    classifier = svm.SVC(C = 10, gamma = .1, kernel='rbf')\n",
    "    classifier.fit(X_train[:s], y_train[:s])\n",
    "    run_time = time.time() - start_time\n",
    "    rbf_runtimes.append(run_time)\n",
    "print(linear_runtimes)\n",
    "print(rbf_runtimes)\n",
    "    \n",
    "np_linear_runtimes = np.array(linear_runtimes)\n",
    "np_rbf_runtimes = np.array(rbf_runtimes)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(sizes, np_linear_runtimes, label = 'linear SVM')\n",
    "plt.scatter(sizes, np_rbf_runtimes, label = 'rbf SVM')\n",
    "plt.xlabel('training set size')\n",
    "plt.ylabel('time to run grid search')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a83674-3fc4-4714-890e-90a25a004c3d",
   "metadata": {},
   "source": [
    "Gammas Tested: Small gamma means each datapoint has a small influence, and a large gamma means each datapoint has a large influence. if gamma='scale' (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma. If auto is used it uses if ‘auto’, uses 1 / n_features for the value of gamma.   \n",
    "\n",
    "C: Used high C values to attempt to classify each training example which may lead to overfitting. We also tested low c values which allows for some miscalssifications but reduces the risk of overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0257912d-ce3d-466d-99bd-4126e3fd00eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    THIS CELL HAS A LONG RUNTIME - several minutes\n",
    "    Choosing hyperparameter for a 3 sec dataset takes upward of 5 minutes, and there are 6 such calls in this cell\n",
    "    The output of this cell usually shows the best accuracy with an rbf kernel using C=100 and gamma='scale'\n",
    "'''\n",
    "# Create grid and choose hyperparameters for each dataset\n",
    "gammas = [.001, .01, 1, 10, 100, 'auto', 'scale']\n",
    "Cs = [ .01, .1, 1, 10, 100]\n",
    "\n",
    "rbf_svm_param_grid = [\n",
    "  {'C': Cs, \n",
    "   'gamma': gammas, \n",
    "   'kernel': ['rbf']},\n",
    " ]\n",
    "linear_svm_param_grid = [\n",
    "  {'C': Cs, \n",
    "   'kernel': ['linear']},\n",
    " ]\n",
    "\n",
    "print(\"run csv 30sec\")\n",
    "rbf_svm_csv30sec_best_estimator_c, rbf_svm_csv30sec_best_estimator_gamma, rbf_svm_csv30sec_accuracy, rbf_svm_csv30sec_val_accuracy, rbf_svm_csv30sec_time = testSVM(\n",
    "    X_30sec_train, y_30sec_train, rbf_svm_param_grid, X_30sec_val, y_30sec_val)\n",
    "\n",
    "print(\"run csv 30sec norm\")\n",
    "rbf_svm_csv30sec_norm_best_estimator_c, rbf_svm_csv30sec_norm_best_estimator_gamma, rbf_svm_csv30sec_norm_accuracy, rbf_svm_csv30sec_norm_val_accuracy, rbf_svm_csv30sec_norm_time = testSVM(\n",
    "    X_30sec_norm_train, y_30sec_train, rbf_svm_param_grid, X_30sec_norm_train, y_30sec_train)\n",
    "\n",
    "print(\"run csv 30sec std\")\n",
    "rbf_svm_csv30sec_std_best_estimator_c, rbf_svm_csv30sec_std_best_estimator_gamma, rbf_svm_csv30sec_std_accuracy, rbf_svm_csv30sec_std_val_accuracy, rbf_svm_csv30sec_std_time = testSVM(\n",
    "    X_30sec_std_train, y_30sec_train, rbf_svm_param_grid, X_30sec_std_val, y_30sec_val)\n",
    "\n",
    "print(\"run csv 3sec\")\n",
    "rbf_svm_csv3sec_best_estimator_c, rbf_svm_csv3sec_best_estimator_gamma, rbf_svm_csv3sec_accuracy, rbf_svm_csv3sec_val_accuracy, rbf_svm_csv3sec_time = testSVM(\n",
    "    X_3sec_train, y_3sec_train, rbf_svm_param_grid, X_3sec_val, y_3sec_val)\n",
    "\n",
    "print(\"run csv 3sec norm\")\n",
    "rbf_svm_csv3sec_norm_best_estimator_c, rbf_svm_csv3sec_norm_best_estimator_gamma, rbf_svm_csv3sec_norm_accuracy, rbf_svm_csv3sec_norm_val_accuracy, rbf_svm_csv3sec_norm_time = testSVM(\n",
    "    X_3sec_norm_train, y_3sec_train, rbf_svm_param_grid, X_3sec_norm_val, y_3sec_val)\n",
    "\n",
    "print(\"run csv 3sec std\")\n",
    "rbf_svm_csv3sec_std_best_estimator_c, rbf_svm_csv3sec_std_best_estimator_gamma, rbf_svm_csv3sec_std_accuracy, rbf_svm_csv3sec_std_val_accuracy, rbf_svm_csv3sec_std_time = testSVM(\n",
    "    X_3sec_std_train, y_3sec_train, rbf_svm_param_grid, X_3sec_std_val, y_3sec_val)\n",
    "\n",
    "print(\"run csv 30sec linear\")\n",
    "svm_csv30sec_best_estimator_c, svm_csv30sec_best_estimator_gamma, svm_csv30sec_accuracy, svm_csv30sec_val_accuracy, svm_csv30sec_time = testSVM(\n",
    "    X_30sec_train, y_30sec_train, linear_svm_param_grid, X_30sec_val, y_30sec_val)\n",
    "\n",
    "print(\"run csv 30sec norm linear\")\n",
    "svm_csv30sec_norm_best_estimator_c, svm_csv30sec_norm_best_estimator_gamma, svm_csv30sec_norm_accuracy, svm_csv30sec_norm_val_accuracy, svm_csv30sec_norm_time = testSVM(\n",
    "    X_30sec_norm_train, y_30sec_train, linear_svm_param_grid, X_30sec_norm_val, y_30sec_val)\n",
    "\n",
    "print(\"run csv 30sec std linear\")\n",
    "svm_csv30sec_std_best_estimator_c, svm_csv30sec_std_best_estimator_gamma, svm_csv30sec_std_accuracy, svm_csv30sec_std_val_accuracy, svm_csv30sec_std_time = testSVM(\n",
    "    X_30sec_std_train, y_30sec_train, linear_svm_param_grid, X_30sec_std_val, y_30sec_val)\n",
    "\n",
    "print(\"run csv 3sec linear\")\n",
    "svm_csv3sec_best_estimator_c, svm_csv3sec_best_estimator_gamma, svm_csv3sec_accuracy, svm_csv3sec_val_accuracy, svm_csv3sec_time = testSVM(\n",
    "    X_3sec_train, y_3sec_train, linear_svm_param_grid, X_3sec_val, y_3sec_val)\n",
    "\n",
    "print(\"run csv 3sec norm linear\")\n",
    "svm_csv3sec_norm_best_estimator_c, svm_csv3sec_norm_best_estimator_gamma, svm_csv3sec_norm_accuracy, svm_csv3sec_norm_val_accuracy, svm_csv3sec_norm_time = testSVM(\n",
    "    X_3sec_norm_train, y_3sec_train, linear_svm_param_grid, X_3sec_norm_val, y_3sec_val)\n",
    "\n",
    "print(\"run csv 3sec std linear\")\n",
    "svm_csv3sec_std_best_estimator_c, svm_csv3sec_std_best_estimator_gamma, svm_csv3sec_std_accuracy, svm_csv3sec_std_val_accuracy, svm_csv3sec_std_time = testSVM(\n",
    "    X_3sec_std_train, y_3sec_train, linear_svm_param_grid, X_3sec_std_val, y_3sec_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb67b0a-a76e-4ee8-ab32-ccc649fd9757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display hyperparameter choice results\n",
    "svmdf = pd.DataFrame({\n",
    "    \"Dataset\": [\"Features 30 sec\", \"Features 30 sec norm\", \"Features 30 sec std\", \"Features 3 sec\", \"Features 3 sec norm\", \"Features 3 sec std\", \"Features 30 sec\", \"Features 30 sec norm\", \"Features 30 sec std\", \"Features 3 sec\", \"Features 3 sec norm\", \"Features 3 sec std\",],\n",
    "    \"Kernel\": [\"rbf\", \"rbf\", \"rbf\", \"rbf\", \"rbf\", \"rbf\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\"],\n",
    "    \"Best C\": [\n",
    "        rbf_svm_csv30sec_best_estimator_c,\n",
    "        rbf_svm_csv30sec_norm_best_estimator_c,\n",
    "        rbf_svm_csv30sec_std_best_estimator_c,\n",
    "        rbf_svm_csv3sec_best_estimator_c,\n",
    "        rbf_svm_csv3sec_norm_best_estimator_c,\n",
    "        rbf_svm_csv3sec_std_best_estimator_c,\n",
    "        svm_csv30sec_best_estimator_c,\n",
    "        svm_csv30sec_norm_best_estimator_c,\n",
    "        svm_csv30sec_std_best_estimator_c\n",
    "        svm_csv3sec_best_estimator_c,\n",
    "        svm_csv3sec_norm_best_estimator_c,\n",
    "        svm_csv3sec_std_best_estimator_c\n",
    "    ],\n",
    "    \"Best gamma\": [\n",
    "        rbf_svm_csv30sec_best_estimator_gamma,\n",
    "        rbf_svm_csv30sec_norm_best_estimator_gamma,\n",
    "        rbf_svm_csv30sec_std_best_estimator_gamma,\n",
    "        rbf_svm_csv3sec_best_estimator_gamma,\n",
    "        rbf_svm_csv3sec_norm_best_estimator_gamma,\n",
    "        rbf_svm_csv3sec_std_best_estimator_gamma,\n",
    "        'na',\n",
    "        'na',\n",
    "        'na',\n",
    "        'na',\n",
    "        'na',\n",
    "        'na'\n",
    "    ],\n",
    "    \"Accuracy\": [\n",
    "        rbf_svm_csv30sec_accuracy,\n",
    "        rbf_svm_csv30sec_norm_accuracy,\n",
    "        rbf_svm_csv30sec_std_accuracy,\n",
    "        rbf_svm_csv3sec_accuracy,\n",
    "        rbf_svm_csv3sec_norm_accuracy,\n",
    "        rbf_svm_csv3sec_std_accuracy,\n",
    "        svm_csv30sec_accuracy,\n",
    "        svm_csv30sec_norm_accuracy,\n",
    "        svm_csv30sec_std_accuracy\n",
    "        svm_csv3sec_accuracy,\n",
    "        svm_csv3sec_norm_accuracy,\n",
    "        svm_csv3sec_std_accuracy\n",
    "    ],\n",
    "    \"Run Time\": [\n",
    "        rbf_svm_csv30sec_time,\n",
    "        rbf_svm_csv30sec_norm_time,\n",
    "        rbf_svm_csv30sec_std_time,\n",
    "        rbf_svm_csv3sec_time,\n",
    "        rbf_svm_csv3sec_norm_time,\n",
    "        rbf_svm_csv3sec_std_time,\n",
    "        svm_csv30sec_time,\n",
    "        svm_csv30sec_norm_time,\n",
    "        svm_csv30sec_std_time\n",
    "        svm_csv3sec_time,\n",
    "        svm_csv3sec_norm_time,\n",
    "        svm_csv3sec_std_time\n",
    "    ]\n",
    "})\n",
    "svmdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cea132a-fec1-4618-8778-2b6651da61ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean accuracy and mean runtime for all runs\n",
    "svm_hp_avg_accuracy = svmdf[\"Accuracy\"].mean()\n",
    "svm_hp_avg_run_time = svmdf[\"Run Time\"].mean()\n",
    "\n",
    "svm_n_hyperparameters_c = 5\n",
    "svm_n_hyperparameters_gamma = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62469afd-ee30-4bbe-b692-179c03b384ff",
   "metadata": {},
   "source": [
    "The choice of (C = 10) and (\\gamma = 0.01) for your SVM model with the RBF kernel provided the best results by effectively balancing the complexity of the decision boundary with the need to generalize well to new data. It is impoortant to note that for this dataset we only used the 30second features and not 3 second due to the exmponential increases when using the 3 second set. \n",
    "\n",
    "The accuracy was the same acros all datasets when using the same kerenl. This was due to the use of the SandardScaler which alwayse standardizes features by removing the mean and scaling to unit variance. It transforms the data so that it has a mean of 0 and a standard deviation of 1. This is particularly useful when the features have different units or variances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6676ab1-6a84-4daf-be20-4626ee12890d",
   "metadata": {},
   "source": [
    "## Random Forest \n",
    "\n",
    "A Random Forest Classifier is a supervised machine learning algorithm that is used for both classification and regression tasks. For this experiment we used it as a classifier.  It operates by constructing a multitude of decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random Forest is known for its high accuracy and robustness, especially in classification tasks. It reduces the risk of overfitting by averaging the results of multiple trees, which lowers the variance and prediction error.\n",
    "\n",
    "While Random Forest is generally efficient, it can be computationally intensive with a large number of trees, which may slow down predictions. The algorithm can require significant memory, especially with large datasets and many trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc764f5-8a60-404f-8083-fce7d3373144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for choosing SVM hyperparameters on various datasets\n",
    "def testRandomForest(features, labels, paramgrid, valFeatures, valLabels):\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    best_estimator, accuracy, run_time = girdSearchClassifier(model, features, labels, paramgrid)\n",
    "    y_pred = best_estimator.predict(valFeatures)\n",
    "    val_accuracy = np.mean(y_pred == valLabels)\n",
    "    \n",
    "    return best_estimator.get_params()['n_estimators'], best_estimator.get_params()['max_depth'], best_estimator.get_params()['bootstrap'], accuracy, val_accuracy, run_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710a8ab9-0101-4ca5-b112-aba140765c5b",
   "metadata": {},
   "source": [
    "#### Random Forest Parameters \n",
    "\n",
    "n-estimators: specifies the number of decision trees in the random forest. increasing the number of trees improves the model's performance because it reduces variance and helps the model generalize better. However, after a certain point, adding more trees yields diminishing returns in terms of accuracy and increases computational cost\n",
    "\n",
    "max_depth: determines the maximum depth of each tree in the forest A deeper tree can capture more information about the data, but it can also lead to overfitting if the depth is too high. Conversely, a shallow tree might underfit the data. A max depth of n allows all the trees to grow until all leaves are pur or until they contain fewer than min_samples_split. We used default min sample split of 2. This was primirly to keep the complexity of the hyper parameter selection to a reasonalble speed for this experiment.\n",
    "\n",
    "bootstrap: Determines weahter whether bootstrap samples are used when building trees. When bootstrapping the next classifier tree will attempt to focus on training examples that were miss classified. It can ensures diversity among trees by using random subsets of data, which helps in reducing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f067a-7c47-4124-a25a-68ca142ca86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid and choose hyperparameters for each dataset\n",
    "random_forest_param_grid = {\n",
    "    'n_estimators': [1, 10, 50, 100, 200],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'bootstrap': [True, False],\n",
    "    'n_jobs': [-1]\n",
    "    \n",
    "}\n",
    "\n",
    "print(\"rf run csv 30sec\")\n",
    "rf_csv30sec_best_estimator_nfeatures, rf_csv30sec_best_estimator_maxdepth, rf_csv30sec_best_estimator_bootstrap, rf_csv30sec_accuracy, rf_csv30sec_val_accuracy, rf_csv30sec_time = testRandomForest(\n",
    "    X_30sec_train, y_30sec_train, random_forest_param_grid, X_30sec_val, y_30sec_val)\n",
    "\n",
    "print(\"rf run csv 30sec norm\")\n",
    "rf_csv30sec_norm_best_estimator_nfeatures, rf_csv30sec_norm_best_estimator_maxdepth, rf_csv30sec_norm_best_estimator_bootstrap, rf_csv30sec_norm_accuracy, rf_csv30sec_norm_val_accuracy, rf_csv30sec_norm_time = testRandomForest(\n",
    "    X_30sec_norm_train, y_30sec_train, random_forest_param_grid, X_30sec_norm_val, y_30sec_val)\n",
    "\n",
    "print(\"rf run csv 30sec std\")\n",
    "rf_csv30sec_std_best_estimator_nfeatures, rf_csv30sec_std_best_estimator_maxdepth, rf_csv30sec_std_best_estimator_bootstrap, rf_csv30sec_std_accuracy, rf_csv30sec_std_val_accuracy, rf_csv30sec_std_time = testRandomForest(\n",
    "    X_30sec_std_train, y_30sec_train, random_forest_param_grid, X_30sec_std_val, y_30sec_val)\n",
    "\n",
    "print(\"rf run csv 3sec \")\n",
    "rf_csv3sec_best_estimator_nfeatures, rf_csv3sec_best_estimator_maxdepth, rf_csv3sec_best_estimator_bootstrap, rf_csv3sec_accuracy, rf_csv3sec_val_accuracy, rf_csv3sec_time = testRandomForest(\n",
    "    X_3sec_train, y_3sec_train, random_forest_param_grid, X_3sec_val, y_3sec_val)\n",
    "\n",
    "print(\"rf run csv 3sec norm \")\n",
    "rf_csv3sec_norm_best_estimator_nfeatures, rf_csv3sec_norm_best_estimator_maxdepth, rf_csv3sec_norm_best_estimator_bootstrap, rf_csv3sec_norm_accuracy, rf_csv3sec_norm_val_accuracy, rf_csv3sec_norm_time = testRandomForest(\n",
    "    X_3sec_norm_train, y_3sec_train, random_forest_param_grid, X_3sec_norm_val, y_3sec_val)\n",
    "\n",
    "print(\"rf run csv 3sec std\")\n",
    "rf_csv3sec_std_best_estimator_nfeatures, rf_csv3sec_std_best_estimator_maxdepth, rf_csv3sec_std_best_estimator_bootstrap, rf_csv3sec_std_accuracy, rf_csv3sec_std_val_accuracy, rf_csv3sec_std_time = testRandomForest(\n",
    "    X_3sec_std_train, y_3sec_train, random_forest_param_grid, X_3sec_std_val, y_3sec_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd44d5d-29b5-4abc-ae63-0357d3453c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display hyperparameter choice results\n",
    "rfdf = pd.DataFrame({\n",
    "    \"Dataset\": [\"Features 30 sec\", \"Features 30 sec norm\", \"Features 30 sec std\", \"Features 3 sec\", \"Features 3 sec norm\", \"Features 3 sec std\"],\n",
    "    \"Best n estimators\": [\n",
    "        rf_csv30sec_best_estimator_nfeatures,\n",
    "        rf_csv30sec_norm_best_estimator_nfeatures,\n",
    "        rf_csv30sec_std_best_estimator_nfeatures,\n",
    "        rf_csv3sec_best_estimator_nfeatures,\n",
    "        rf_csv3sec_norm_best_estimator_nfeatures,\n",
    "        rf_csv3sec_std_best_estimator_nfeatures\n",
    "    ],\n",
    "    \"Best Max Depth\": [\n",
    "        rf_csv30sec_best_estimator_maxdepth,\n",
    "        rf_csv30sec_norm_best_estimator_maxdepth,\n",
    "        rf_csv30sec_std_best_estimator_maxdepth,\n",
    "        rf_csv3sec_best_estimator_maxdepth ,\n",
    "        rf_csv3sec_norm_best_estimator_maxdepth,\n",
    "        rf_csv3sec_std_best_estimator_maxdepth\n",
    "    ],\n",
    "    \"Bootstrap\": [\n",
    "        rf_csv30sec_best_estimator_bootstrap,\n",
    "        rf_csv30sec_norm_best_estimator_bootstrap,\n",
    "        rf_csv30sec_std_best_estimator_bootstrap,\n",
    "        rf_csv3sec_best_estimator_bootstrap,\n",
    "        rf_csv3sec_norm_best_estimator_bootstrap,\n",
    "        rf_csv3sec_std_best_estimator_bootstrap\n",
    "    ],\n",
    "    \"Accuracy\": [\n",
    "        rf_csv30sec_accuracy,\n",
    "        rf_csv30sec_norm_accuracy,\n",
    "        rf_csv30sec_std_accuracy,\n",
    "        rf_csv3sec_accuracy,\n",
    "        rf_csv3sec_norm_accuracy,\n",
    "        rf_csv3sec_std_accuracy\n",
    "    ],\n",
    "    \"Run Time\": [\n",
    "        rf_csv30sec_time,\n",
    "        rf_csv30sec_norm_time,\n",
    "        rf_csv30sec_std_time,\n",
    "        rf_csv3sec_time,\n",
    "        rf_csv3sec_norm_time,\n",
    "        rf_csv3sec_std_time\n",
    "    ]\n",
    "})\n",
    "rfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa494f1-ba0c-4eac-8c52-3f53d569191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean accuracy and mean runtime for all runs\n",
    "rfdf_hp_avg_accuracy = rfdf[\"Accuracy\"].mean()\n",
    "rfdf_hp_avg_run_time = rfdf[\"Run Time\"].mean()\n",
    "\n",
    "rf_n_hyperparameters_n_estimators = 5\n",
    "rf_n_hyperparameters_max_depth = 4\n",
    "rf_n_hyperparameters_bootstrap = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d49cd93-322d-4048-88f4-38ea859bddbe",
   "metadata": {},
   "source": [
    "The results indicate that the best performance was boostrap = false, max_depth = none, and n_estimators  = none. When bootstrap=False, the model does not use bootstrapped samples (sampling with replacement) to train each tree. Instead, it uses the entire dataset for each tree. The max depth setting  can capture complex patterns in the data by allowing trees to grow fully. It is useful when the dataset is complex and requires deep trees to model intricate relationships. It can potentially increase the risk of overfitting. \n",
    "\n",
    "200 trees were used to keep the runtime down, exploring more trees could potentially improve performance further, especially if computational resources allow for it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9947bcd2-18da-4039-a8b9-2242f825150a",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5ea54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Check TensorFlow setup'''\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaa01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    THIS CELL HAS A LONG RUNTIME - upwards of several minutes\n",
    "    By using this command we determined that the model seems to work best when\n",
    "    using kernel size 32 for layer 1, kernel size 16 for layer 2, and no layer 3.\n",
    "    Moving forward we will use this CNN configuration to cross-validate and choose hyperparameters\n",
    "'''\n",
    "def choose_CNN(l1, l2, l3):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(l1, (3, 3), activation='relu', input_shape=(288, 432, 4)))\n",
    "    if l2 != None:\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Conv2D(l2, (3, 3), activation='relu'))\n",
    "    if l3 != None:\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Conv2D(l3, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(10))\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "'''Train and test CNN'''\n",
    "X_images_train_tens = tf.convert_to_tensor(X_images_train, dtype=float)\n",
    "y_images_train_tens = tf.convert_to_tensor(y_images_train.astype(np.float32), dtype=float)\n",
    "X_images_val_tens = tf.convert_to_tensor(X_images_val, dtype=float)\n",
    "y_images_val_tens = tf.convert_to_tensor(y_images_val.astype(np.float32), dtype=float)\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "history = model.fit(X_images_train_tens, y_images_train_tens, epochs=1, validation_data=(X_images_val_tens, y_images_val_tens))\n",
    "\n",
    "layer1 = [16,32,64]\n",
    "layer2 = [None,16,32]\n",
    "layer3 = [None,16,32]\n",
    "allModels = []\n",
    "\n",
    "for l1 in layer1:\n",
    "    for l2 in layer2:\n",
    "        for l3 in layer3:\n",
    "            allModels.append(choose_CNN(l1,l2,l3))\n",
    "\n",
    "for m in allModels:\n",
    "    m.evaluate(X_images_val_tens,  y_images_val_tens, verbose=2)\n",
    "\n",
    "cell_stop = time.time()\n",
    "\n",
    "cnn_run_time = cell_stop-cell_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec79be3-6b3c-40f3-867f-a713c4d6cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    THIS CELL HAS A LONG RUNTIME - upwards of several minutes\n",
    "    By using this command we determined that the best optimizer for nearly all amounts of epochs was adamax\n",
    "    Adamax performed best at 10 epochs\n",
    "'''\n",
    "\n",
    "cnnstart = time.time()\n",
    "def train_test_cnn(opt,epch):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(288, 432, 4)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(16, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10),\n",
    "    ])\n",
    "    model.compile(optimizer=opt,loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "    model.fit(X_images_train_tens, y_images_train_tens, epochs=epch)\n",
    "    _, accuracy = model.evaluate(X_images_val_tens, y_images_val_tens)\n",
    "    return accuracy\n",
    "\n",
    "# Convert spectrograms to tensors\n",
    "X_images_train_tens = tf.convert_to_tensor(X_images_train, dtype=float)\n",
    "y_images_train_tens = tf.convert_to_tensor(y_images_train.astype(np.float32), dtype=float)\n",
    "X_images_val_tens = tf.convert_to_tensor(X_images_val, dtype=float)\n",
    "y_images_val_tens = tf.convert_to_tensor(y_images_val.astype(np.float32), dtype=float)\n",
    "\n",
    "# Set up cross-validation and choose hyperparameters\n",
    "OPTIMIZER = ['adam','adamax','ftrl','rmsprop','sgd']\n",
    "EPOCHS = [5,10,20,30,40]\n",
    "\n",
    "session_num = 0\n",
    "\n",
    "outputs = []\n",
    "cnntime = 0\n",
    "cnnstart = time.time()\n",
    "for optimizer in OPTIMIZER:\n",
    "    for epoch in EPOCHS:\n",
    "        tempAcc = train_test_cnn(optimizer, epoch)\n",
    "        outputs.append('Trial Number: ' + str(session_num) + \n",
    "                    '\\nOptimizer: ' + optimizer + \n",
    "                    '\\nEpochs: ' + str(epoch) +\n",
    "                    '\\nAccuracy: ' + str(tempAcc))\n",
    "        session_num += 1\n",
    "\n",
    "cnnstop = time.time()\n",
    "cnntime = cnnstop-cnnstart\n",
    "for result in outputs:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93e9f32-b369-4c2e-89b8-2dff753be650",
   "metadata": {},
   "source": [
    "## Classifier Compairsion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d949f80-38fd-4fa7-bced-bdccfca254ff",
   "metadata": {},
   "source": [
    "#### Baseline Classifier the Random Classifier\n",
    "\n",
    "The Random classifier is a baseline classification model that will predict the occuring class randomly, and unifomly in the dataset for all inputs, regardless of the features. If the other classifiers can not perform better than the random classifer they are underperforming and not estimating results. Naive classifiers give a minimum performance threshold. If your sophisticated model can't outperform a naive approach, it indicates that the model might be overfitting or underfitting. It also might indicate that features or the overall dataset might not be informative enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ed4e8c-c008-4e84-8e6d-651c9c91639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a dummy classifier to generate a baseline for model performance\n",
    "dummy_results = []\n",
    "\n",
    "random_classifier = DummyClassifier(strategy=\"uniform\", random_state=42)\n",
    "for X, y, data_set in features_datasets:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "    fit_start_time = time.time()\n",
    "    random_classifier.fit(X_train, y_train)\n",
    "    fit_stop_time = time.time()\n",
    "    y_pred=random_classifier.predict(X_test)\n",
    "    predict_stop_time = time.time()\n",
    "    classifier_accuracy = np.mean(y_pred == y_test)\n",
    "    fit_time = fit_stop_time - fit_start_time\n",
    "    predict_time = predict_stop_time - fit_stop_time\n",
    "    dummy_results.append([data_set, random_classifier.__class__.__name__, fit_time, predict_time, classifier_accuracy])\n",
    "\n",
    "dummy_df = pd.DataFrame(dummy_results, columns=[\"Dataset\", \"Classifier\", \"Fit Time (s)\", \"Predict Time (s)\", \"Accuracy\"])\n",
    "dummy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787aa601-6129-431b-a9a0-343c9d183134",
   "metadata": {},
   "source": [
    "The random classifier achieved an average accuracy of approximately 10% across all datasets. This outcome aligns with expectations, given the fact that the each lable has a uniform distribution that represent 10% of the samples from each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f2244b",
   "metadata": {},
   "source": [
    "## Metric Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98e45ee-1413-4275-b06d-b29541d10929",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create all best models\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "svm_classifier = svm.SVC(C=100, gamma = 'scale', kernel=\"rbf\")\n",
    "rf_classifier = RandomForestClassifier(n_estimators = 200, max_depth = 20, bootstrap = False, n_jobs = -1)\n",
    "cnn_classifier = tf.keras.models.Sequential([\n",
    "                    layers.Conv2D(32, (3, 3), activation='relu', input_shape = (288, 432, 4)),\n",
    "                    layers.MaxPooling2D((2, 2)),\n",
    "                    layers.Conv2D(16, (3, 3), activation='relu'),\n",
    "                    layers.Flatten(),\n",
    "                    layers.Dense(64, activation='relu'),\n",
    "                    layers.Dense(10),\n",
    "                ])\n",
    "cnn_classifier.compile(optimizer='adamax',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "\n",
    "# Compare runtimes during hyperparameter selection\n",
    "knn_hypertimes = [knn_csv30sec_time,\n",
    "                knn_csv30sec_norm_time,\n",
    "                knn_csv30sec_std_time,\n",
    "                knn_csv3sec_time,\n",
    "                knn_csv3sec_norm_time,\n",
    "                knn_csv3sec_std_time]\n",
    "svm_hypertimes = [svm_csv30sec_time,\n",
    "                       svm_csv30sec_norm_time,\n",
    "                       svm_csv30sec_std_time,\n",
    "                       svm_csv3sec_time,\n",
    "                       svm_csv3sec_norm_time,\n",
    "                       svm_csv3sec_std_time]\n",
    "rbf_hypertimes = [rbf_svm_csv30sec_time,\n",
    "                    rbf_svm_csv30sec_norm_time,\n",
    "                    rbf_svm_csv30sec_std_time,\n",
    "                    rbf_svm_csv3sec_time,\n",
    "                    rbf_svm_csv3sec_norm_time,\n",
    "                    rbf_svm_csv3sec_std_time,]\n",
    "rf_hypertimes = [rf_csv30sec_time,\n",
    "               rf_csv30sec_norm_time,\n",
    "               rf_csv30sec_std_time,\n",
    "               rf_csv3sec_time,\n",
    "               rf_csv3sec_norm_time,\n",
    "               rf_csv3sec_std_time]\n",
    "\n",
    "# Compare train times\n",
    "knn_traintimes = []\n",
    "svm_traintimes = []\n",
    "rf_traintimes = []\n",
    "for i in [(X_30sec_fulltrain, y_30sec_fulltrain),(X_30sec_norm_fulltrain, y_30sec_fulltrain),(X_30sec_std_fulltrain, y_30sec_fulltrain),(X_3sec_fulltrain, y_3sec_fulltrain),(X_3sec_norm_fulltrain, y_3sec_fulltrain),(X_3sec_std_fulltrain, y_3sec_fulltrain)]:\n",
    "    temp = %timeit -o knn_classifier.fit(i[0],i[1])\n",
    "    knn_traintimes.append(int(temp.average * 1e9))\n",
    "    temp = %timeit -o svm_classifier.fit(i[0],i[1])\n",
    "    svm_traintimes.append(int(temp.average * 1e9))\n",
    "    temp = %timeit -o rf_classifier.fit(i[0],i[1])\n",
    "    rf_traintimes.append(int(temp.average * 1e9))\n",
    "cnn_traintimes = []\n",
    "X_images_fulltrain_tens = tf.convert_to_tensor(X_images_fulltrain, dtype=float)\n",
    "y_images_fulltrain_tens = tf.convert_to_tensor(y_images_fulltrain.astype(np.float32), dtype=float)\n",
    "X_images_test_tens = tf.convert_to_tensor(X_images_test, dtype=float)\n",
    "y_images_test_tens = tf.convert_to_tensor(y_images_test.astype(np.float32), dtype=float)\n",
    "temp = %timeit -r 1 -n 1 -o cnn_classifier.fit(X_images_fulltrain_tens, y_images_fulltrain_tens, epochs=10)\n",
    "cnn_traintimes.append(int(temp.average))\n",
    "\n",
    "# Print class names\n",
    "print('\\nClass names: ')\n",
    "print(genres)\n",
    "\n",
    "# Compare confusion matrices and other metrics\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "svm_classifier = svm.SVC(C=100, gamma = .001, kernel=\"rbf\")\n",
    "rf_classifier = RandomForestClassifier(n_estimators = 200, max_depth = 20, bootstrap = False, n_jobs = -1)\n",
    "\n",
    "knn_classifier.fit(X_3sec_norm_fulltrain, y_3sec_fulltrain)\n",
    "y_pred = knn_classifier.predict(X_3sec_norm_test)\n",
    "knn_confusion = confusion_matrix(y_3sec_test, y_pred)\n",
    "FP = knn_confusion.sum(axis=0) - np.diag(knn_confusion)\n",
    "FN = knn_confusion.sum(axis=1) - np.diag(knn_confusion)\n",
    "TP = np.diag(knn_confusion)\n",
    "knn_precision = np.mean(TP/(TP+FP))\n",
    "knn_recall = np.mean(TP/(TP+FN))\n",
    "knn_f1 = (2*knn_precision*knn_recall)/(knn_precision+knn_recall)\n",
    "knn_acc = np.mean(y_3sec_test == y_pred)\n",
    "print('\\nKNN Confusion Matrix')\n",
    "print(knn_confusion)\n",
    "print('KNN F1 Score - ' + str(knn_f1))\n",
    "print('KNN Precision Score - ' + str(knn_precision))\n",
    "print('KNN Recall Score - ' + str(knn_recall))\n",
    "print('KNN Accuracy - ' + str(knn_acc))\n",
    "\n",
    "svm_classifier.fit(X_3sec_std_fulltrain, y_3sec_fulltrain)\n",
    "y_pred = svm_classifier.predict(X_3sec_std_test)\n",
    "svm_confusion = confusion_matrix(y_3sec_test, y_pred)\n",
    "FP = svm_confusion.sum(axis=0) - np.diag(svm_confusion)\n",
    "FN = svm_confusion.sum(axis=1) - np.diag(svm_confusion)\n",
    "TP = np.diag(svm_confusion)\n",
    "svm_precision = np.mean(TP/(TP+FP))\n",
    "svm_recall = np.mean(TP/(TP+FN))\n",
    "svm_f1 = (2*svm_precision*svm_recall)/(svm_precision+svm_recall)\n",
    "svm_acc = np.mean(y_3sec_test == y_pred)\n",
    "print('\\nSVM Confusion Matrix')\n",
    "print(svm_confusion)\n",
    "print('SVM F1 Score - ' + str(svm_f1))\n",
    "print('SVM Precision Score - ' + str(svm_precision))\n",
    "print('SVM Recall Score - ' + str(svm_recall))\n",
    "print('SVM Accuracy - ' + str(svm_acc))\n",
    "\n",
    "rf_classifier.fit(X_3sec_std_fulltrain, y_3sec_fulltrain)\n",
    "y_pred = rf_classifier.predict(X_3sec_std_test)\n",
    "rf_confusion = confusion_matrix(y_3sec_test, y_pred)\n",
    "FP = rf_confusion.sum(axis=0) - np.diag(rf_confusion)\n",
    "FN = rf_confusion.sum(axis=1) - np.diag(rf_confusion)\n",
    "TP = np.diag(rf_confusion)\n",
    "rf_precision = np.mean(TP/(TP+FP))\n",
    "rf_recall = np.mean(TP/(TP+FN))\n",
    "rf_f1 = (2*rf_precision*rf_recall)/(rf_precision+rf_recall)\n",
    "rf_acc = np.mean(y_3sec_test == y_pred)\n",
    "print('\\nRandom Forest Confusion Matrix')\n",
    "print(rf_confusion)\n",
    "print('Random Forest F1 Score - ' + str(rf_f1))\n",
    "print('Random Forest Precision Score - ' + str(rf_precision))\n",
    "print('Random Forest Recall Score - ' + str(rf_recall))\n",
    "print('Random Forest Accuracy - ' + str(rf_acc))\n",
    "\n",
    "y_pred = cnn_classifier.predict(X_images_test)\n",
    "y_pred_classes = np.array([])\n",
    "for i in range(len(y_pred)):\n",
    "    y_pred_classes = np.append(y_pred_classes, np.argmax(y_pred[i]))\n",
    "cnn_confusion = tf.math.confusion_matrix(y_images_test.astype(int), y_pred_classes.astype(int)).numpy()\n",
    "FP = np.sum(cnn_confusion.sum(axis=0) - np.diag(cnn_confusion))\n",
    "FN = np.sum(cnn_confusion.sum(axis=1) - np.diag(cnn_confusion))\n",
    "TP = np.sum(np.diag(cnn_confusion))\n",
    "cnn_precision = np.mean(TP/(TP+FP))\n",
    "cnn_recall = np.mean(TP/(TP+FN))\n",
    "cnn_f1 = (2*cnn_precision*cnn_recall)/(cnn_precision+cnn_recall)\n",
    "cnn_acc = np.mean(y_images_test.astype(int) == y_pred_classes.astype(int))\n",
    "print('\\nConvolutional Neural Network Confusion Matrix')\n",
    "print(cnn_confusion)\n",
    "print('Convolutional Neural Network F1 Score - ' + str(cnn_f1))\n",
    "print('Convolutional Neural Network Precision Score - ' + str(cnn_precision))\n",
    "print('Convolutional Neural Network Recall Score - ' + str(cnn_recall))\n",
    "print('Convolutional Neural Network Accuracy - ' + str(cnn_acc))\n",
    "\n",
    "# Plot results\n",
    "bar1 = np.arange(len(knn_traintimes))\n",
    "bar2 = [x + 0.2 for x in bar1]\n",
    "bar3 = [x + 0.2 for x in bar2]\n",
    "bar4 = [x + 0.2 for x in bar3]\n",
    "\n",
    "fig = plt.subplots(figsize = (12, 8))\n",
    "plt.bar(bar1, knn_hypertimes, color = 'r', width = 0.2, edgecolor = 'black', label = 'knn')\n",
    "plt.bar(bar2, svm_hypertimes, color = 'b', width = 0.2, edgecolor = 'black', label = 'linear svm')\n",
    "plt.bar(bar3, rbf_hypertimes, color = 'g', width = 0.2, edgecolor = 'black', label = 'rbf svm')\n",
    "plt.bar(bar4, rf_hypertimes, color = 'm', width = 0.2, edgecolor = 'black', label = 'rf')\n",
    "plt.title('Hyperparameter selection times', fontsize = 15)\n",
    "plt.ylabel('Time to Run (seconds)', fontsize = 18)\n",
    "plt.xlabel('Dataset', fontsize = 18)\n",
    "plt.xticks([x + 0.3 for x in range(len(bar1))], ['X_30sec', 'X_30sec_norm', 'X_30sec_std', 'X_3sec', 'X_3sec_norm', 'X_3sec_std'])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig1 = plt.subplots(figsize = (12, 8))\n",
    "plt.bar(bar1, knn_traintimes, color = 'r', width = 0.2, edgecolor = 'black', label = 'knn')\n",
    "plt.bar(bar2, svm_traintimes, color = 'b', width = 0.2, edgecolor = 'black', label = 'svm')\n",
    "plt.bar(bar3, rf_traintimes, color = 'g', width = 0.2, edgecolor = 'black', label = 'rf')\n",
    "plt.title('Training times', fontsize = 15)\n",
    "plt.ylabel('Time to Train (milliseconds)', fontsize = 18)\n",
    "plt.xlabel('Dataset', fontsize = 18)\n",
    "plt.xticks(bar2, ['X_30sec', 'X_30sec_norm', 'X_30sec_std', 'X_3sec', 'X_3sec_norm', 'X_3sec_std'])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Convolutional Neural Network Train Time: ' + str(cnn_traintimes[0]) + ' (seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a84a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y, data_set in features_datasets:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "    for classifier in features_classifiers:\n",
    "        fit_start_time = time.time()\n",
    "        classifier.fit(X_train, y_train)\n",
    "        fit_stop_time = time.time()\n",
    "        y_pred=classifier.predict(X_test)\n",
    "        predict_stop_time = time.time()\n",
    "        classifier_accuracy = np.mean(y_pred == y_test)\n",
    "        fit_time = fit_stop_time - fit_start_time\n",
    "        predict_time = predict_stop_time - fit_stop_time\n",
    "        features_results.append([data_set, classifier.__class__.__name__, fit_time, predict_time, classifier_accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2327bbd5-e345-49fc-92a9-08bd087a7e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_mapping = {\n",
    "    \"RandomForestClassifier\": \"RF\",\n",
    "    \"SupportVectorClassifier\": \"SVM\",\n",
    "    \"KNeighborsClassifier\": \"KNN\",\n",
    "    \"DummyClassifier\": \"Random\",\n",
    "    \"ConvolutionalNeuralNetwork\": \"CNN\",\t\n",
    "}\n",
    "results_df[\"Classifier\"] = results_df[\"Classifier\"].replace(name_mapping)\n",
    "columns = [\"Dataset\", \"Classifier\", \"Fit Time (s)\", \"Predict Time (s)\", \"Accuracy\"]\n",
    "results_df = pd.DataFrame(features_results, columns=columns)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09db180b-6698-4ac0-9522-c14915733e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_averages = {}\n",
    "for dataset_name in results_df[\"Dataset\"].unique():\n",
    "    filtered_df = results_df[results_df[\"Dataset\"] == dataset_name]\n",
    "    averages = filtered_df[[\"Fit Time (s)\", \"Predict Time (s)\", \"Accuracy\"]].mean()\n",
    "    dataset_averages[dataset_name] = averages\n",
    "dataset_averages_df = pd.DataFrame.from_dict(dataset_averages, orient=\"index\")\n",
    "dataset_averages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b000000d-1f8d-41ff-b67b-cfd33e0b4d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f6a6f5-e861-4bdc-8ade-eff27e34128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_averages = {}\n",
    "for classifier_name in results_df[\"Classifier\"].unique():\n",
    "    filtered_df = results_df[results_df[\"Classifier\"] == classifier_name]\n",
    "    averages = filtered_df[[\"Fit Time (s)\", \"Predict Time (s)\", \"Accuracy\"]].mean()\n",
    "    classifier_averages[classifier_name] = averages.to_dict()  # Convert Series to dict\n",
    "classifier_averages_df = pd.DataFrame.from_dict(classifier_averages, orient=\"index\")\n",
    "classifier_averages_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b7106-e565-4f63-bab9-1f08aa9a2206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533135e1-15c6-4c9b-9a51-ea38224f3db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd13e72c-583f-4b87-ab1a-70ce1c4bba6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
