{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181e235d-06c1-4b0d-b714-ef59ef8410bb",
   "metadata": {},
   "source": [
    "# Music Genera Classification With the GTZAN dataset\n",
    "CS345 Fall 2024 Project   \n",
    "Wade McCaulley  \n",
    "Jacob Ingraham  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a98ce2c5-e32c-4b8e-a55b-ecb3acc9460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cdc4e79-3859-4bcb-9e02-c04b647c2a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_30_seconds_filepath = \"../Data/features_30_sec.csv\"\n",
    "features_3_seconds_filepath = \"../Data/features_3_sec.csv\"\n",
    "mel_spectrograms_filepath = \"../Data/images_original\"\n",
    "\n",
    "genres = [\"blues\", \"classical\" , \"country\", \"disco\", \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1ceb96-8933-4c5f-9990-6af0f350244a",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "10 generes, 100 audio files each with a length of 30 seconds  \n",
    "The dataset has been refered to as \"The MNIST of sounds\", which copares it to the database of handwritten digits used throughout CS345  \n",
    "\n",
    "Music Generes:\n",
    "* Blues\n",
    "* Classical\n",
    "* Country\n",
    "* Disco\n",
    "* Hiphop\n",
    "* Jazz\n",
    "* Metal\n",
    "* Pop\n",
    "* Reggae\n",
    "* Rock\n",
    "\n",
    "#### Images\n",
    "Images are a visual representation of each audio file in the form of a Mel Spectrogram. A Mel Spectrogram converts audio signals into a visual format that highlights frequency and amplitude over the 30 second audio file. Mel Spectrograms are used to represent audio signals in a way that aligns with human preception of sounds. The frequency axis is transformed into the Mels Scale wich can be captured digitally as a waveform which can then be used by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "906f4bc1-2f5d-4cfa-a837-2a3ce5d17674",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''loads the CSVs. Features are everything but the first col(filename), and the lables. The lables are the last column''' \n",
    "def loadCSVs(filepath):\n",
    "    data = pd.read_csv(filepath, dtype = object, delimiter = ',').values\n",
    "    X = data[:,2:-1]\n",
    "    y = data[:,-1:]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f664227-1b71-494d-abea-47dfa3d340a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This will turn the genere lables into np.array of ints'''\n",
    "def lable_to_int(lables, genres):\n",
    "    lable_int = np.array(lables)\n",
    "    for i in range(len(genres)):\n",
    "        lable_int[lable_int==genres[i]]=i\n",
    "    return lable_int \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66b3e3d7-b630-486c-b4de-df74a1dec118",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#this will turn the png where each pixel is represented by 4 values into a single value. The first three are colors, and I think the forth is transparancy.'''\n",
    "def gray_scale_images(images):\n",
    "    gray_images = np.dot(images[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "    return np.array(gray_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdb525d5-d3cf-43a6-947b-974aff3e4a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''loads the mel spectrograms into a np array of images. Each image is 288, 432 pixels, and each pixel is represented by four values'''\n",
    "def load_mel_spectrograms():\n",
    "    image_features = []\n",
    "    image_lables = []\n",
    "    for genre in genres:\n",
    "        print(\"Loading\", genre)\n",
    "        images_file_path = mel_spectrograms_filepath + \"/\" + genre\n",
    "        png_files = [f for f in os.listdir(images_file_path) if f.endswith('.png')]\n",
    "\n",
    "        for file in png_files:\n",
    "            file_path = images_file_path +\"/\"+ file\n",
    "            image = plt.imread(file_path)  # Load the image\n",
    "            image_features.append(image)\n",
    "            image_lables.append(genre)\n",
    "\n",
    "    return np.array(image_features), np.array(image_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea624ba4-f7df-44e0-920c-4bec63122ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 57) (1000,)\n",
      "(9990, 57) (9990,)\n"
     ]
    }
   ],
   "source": [
    "string_X_30sec, y_30sec = loadCSVs(features_30_seconds_filepath)\n",
    "X_30sec = string_X_30sec.astype(np.float64)\n",
    "string_X_3sec, y_3sec = loadCSVs(features_3_seconds_filepath)\n",
    "X_3sec = string_X_3sec.astype(np.float64)\n",
    "\n",
    "# reshape y to a 1d array\n",
    "y_30sec = y_30sec.ravel()\n",
    "y_3sec = y_3sec.ravel()\n",
    "\n",
    "print(X_30sec.shape, y_30sec.shape)\n",
    "print(X_3sec.shape, y_3sec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e833a67-c7af-4aac-b059-fafbcac845eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading blues\n",
      "Loading classical\n",
      "Loading country\n",
      "Loading disco\n",
      "Loading hiphop\n",
      "Loading jazz\n",
      "Loading metal\n",
      "Loading pop\n",
      "Loading reggae\n",
      "Loading rock\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((999, 288, 432, 4), (999,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_images, y_images = load_mel_spectrograms()\n",
    "y_images = lable_to_int(y_images,genres)\n",
    "X_images.shape, y_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93bc9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_images_train, X_images_temp, y_images_train, y_images_temp = train_test_split(X_images, y_images, test_size=0.2, shuffle=True, random_state=7)\n",
    "X_images_val, X_images_test, y_images_val, y_images_test = train_test_split(X_images_temp, y_images_temp, test_size=0.5, shuffle=True, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240058da-7c2b-4ef8-aadc-9a79df7d1fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grayscale images \n",
    "X_images_gray = gray_scale_images(X_images)\n",
    "X_images_gray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7fab7f-0d3b-4598-98f7-503e461ecfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_30sec_int = lable_to_int(y_30sec, genres)\n",
    "y_3sec_int = lable_to_int(y_3sec, genres)\n",
    "y_images_int = lable_to_int(y_images, genres)\n",
    "y_30sec_int.shape, y_3sec_int.shape, y_images_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7eb730-7c49-46a8-ba43-90d96b6354e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create normalized and standardized versions of data'''\n",
    "X_30sec_norm = (X_30sec-np.min(X_30sec, axis=0))/(np.max(X_30sec,axis=0)-np.min(X_30sec,axis=0))\n",
    "X_3sec_norm = (X_3sec-np.min(X_3sec, axis=0))/(np.max(X_3sec,axis=0)-np.min(X_3sec,axis=0))\n",
    "X_30sec_std = (X_30sec-np.mean(X_30sec, axis=0))/(np.std(X_30sec, axis=0))\n",
    "X_3sec_std = (X_3sec-np.mean(X_3sec, axis=0))/(np.std(X_3sec, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bdc6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Check norm and std data'''\n",
    "print(np.max(X_30sec_norm)==1,np.min(X_30sec_norm)==0)\n",
    "print(np.max(X_3sec_norm)==1,np.min(X_3sec_norm)==0)\n",
    "print(np.mean(X_30sec_std), np.std(X_30sec_std))\n",
    "print(np.mean(X_3sec_std), np.std(X_3sec_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c29eb03-d02c-4959-acbd-c26ad1c8af85",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "stratified K Fold cross validation with shuffling to make sure each fold isnt made up on consecurtive samples. The CSV and Images objects were loaded sequentially. Each fold should maintain approximatly the same number from each of the generes, which can prevent some classes from being represeneted in certain folds. \n",
    "\n",
    "used the accuracy_score for corss validation. When accuracy_score is selected it will \n",
    "\n",
    "This step was extremly slow. \n",
    "Wanted to set hyper parameters for each model in a similar way. \n",
    "\n",
    "k fold cross validation has a run time of $O(n)$ where $n$ is the sample size, and $k$ is the number of folds. \n",
    "Selecting 5 folds. reduces computational complexity increasing run times. May risk overfitting, if one split has significantly better performance. Fewer folds may result in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb5e714-32ce-470f-86d6-a33a23a768ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cb6aea-915a-4cf9-89b4-e8d8aa9d4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def girdSearchClassifier(model, features, labels, paramgrid):\n",
    "    start_time = time.time()\n",
    "    cv = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "    classifier = GridSearchCV(model, paramgrid)\n",
    "    classifier.fit(features, labels)\n",
    "    accuracies = cross_val_score(classifier.best_estimator_, features, labels, cv=cv, \n",
    "                           scoring='accuracy')\n",
    "    accuracy = np.mean(accuracies)\n",
    "    run_time = time.time() - start_time\n",
    "\n",
    "\n",
    "    return classifier.best_estimator_, accuracy, run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fea6121-029d-4f37-8a02-a2325084fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_datasets_30sec = [\n",
    "    (X_30sec, y_30sec, \"Features 30 sec\"),\n",
    "    (X_30sec_norm, y_30sec, \"Features 30 sec norm\"),\n",
    "    (X_30sec_std, y_30sec, \"Features 30 sec std\")\n",
    "]\n",
    "\n",
    "features_datasets_3sec = [\n",
    "    (X_3sec, y_3sec, \"Features 3 sec\"),\n",
    "    (X_3sec_norm, y_3sec, \"Features 3 sec norm\"),\n",
    "    (X_3sec_std, y_3sec, \"Features 3 sec std\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f2d422-f7ad-4e6a-bc00-be8afb3f5523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_results(test_classifier, X, y, dataset_name, param_grid, results):\n",
    "    best_estimators, accuracy, run_time = test_classifier(X, y, param_grid)\n",
    "    result = {\n",
    "        \"Dataset\": dataset_name,\n",
    "        \"Model\": test_classifier.__name__,\n",
    "        \"Best Params\": best_estimators,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Run Time\": run_time\n",
    "    }\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a72e48-78da-46e4-87c1-4a51240ec4c0",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f13fe-b3df-417b-912c-5afa7531741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just a place holder if we want to do it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed679947-de17-4120-8bd7-e6297157328b",
   "metadata": {},
   "source": [
    "## Nearest Neighbor \n",
    "The Nearest Neighbor classifier works by finding an example in the training data that features most closely match the features that need to be classified. It then returns the lable of this closest features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be87950-99dc-4e5b-abf2-70666ef796b9",
   "metadata": {},
   "source": [
    "#### Nearest Neighbor Hyperparameters\n",
    "\n",
    "number of neighbors: n_neighbors: This is the most critical hyper parameter and relates to how many neighbors are considered when making a prediction.  \n",
    "\n",
    "The nearest neighbor classifier can be improved by making the classification on multiple different neighbors. For this testing we are using the k-nn classifer that compares the $k$ nearest neibhors. \n",
    "\n",
    "\n",
    "#### Nearest Neighbor Running Time\n",
    "The heares neighbor classifier has a running time of $O(N * d)$ where n is the number of training examples and d is the number of dimensions in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f09c880-b8d1-4437-98de-4988901bbb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def testKNN(features, labels, paramgrid):\n",
    "\n",
    "    model = KNeighborsClassifier()\n",
    "    best_estimator, accuracy, run_time = girdSearchClassifier(model, features, labels, paramgrid)\n",
    "    \n",
    "    return best_estimator.get_params()['n_neighbors'], accuracy, run_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b05fd25-1c74-429e-8939-ed83c3502e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_param_grid = {\n",
    "    'n_neighbors': [1,2,4,8,16,32,64,128, 256, 512]\n",
    "}\n",
    "print(\"knn run csv 30sec\")\n",
    "knn_csv30sec_best_estimator, knn_csv30sec_accuracy, knn_csv30sec_time = testKNN(X_30sec, y_30sec, knn_param_grid)\n",
    "\n",
    "print(\"run csv 30sec norm\")\n",
    "knn_csv30sec_norm_best_estimator, knn_csv30sec_norm_accuracy, knn_csv30sec_norm_time = testKNN(X_30sec_norm, y_30sec, knn_param_grid)\n",
    "\n",
    "print(\"run csv 30sec std\")\n",
    "knn_csv30sec_std_best_estimator, knn_csv30sec_std_accuracy, knn_csv30sec_std_time = testKNN(X_30sec_std, y_30sec, knn_param_grid)\n",
    "\n",
    "print(\"run csv 3sec\")\n",
    "knn_csv3sec_best_estimator, knn_csv3sec_accuracy, knn_csv3sec_time = testKNN(X_3sec, y_3sec, knn_param_grid)\n",
    "\n",
    "print(\"run csv 3sec norm\")\n",
    "knn_csv3sec_norm_best_estimator, knn_csv3sec_norm_accuracy, knn_csv3sec_norm_time = testKNN(X_3sec_norm, y_3sec, knn_param_grid)\n",
    "\n",
    "print(\"run csv 3sec std\")\n",
    "knn_csv3sec_std_best_estimator, knn_csv3sec_std_accuracy, knn_csv3sec_std_time = testKNN(X_3sec_std, y_3sec, knn_param_grid)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Dataset\": [\"Features 30 sec\", \"Features 30 sec norm\", \"Features 30 sec std\", \"Features 3 sec\", \"Features 3 sec norm\", \"Features 3 sec std\"],\n",
    "    \"Best n_neighbors\": [\n",
    "        knn_csv30sec_best_estimator,\n",
    "        knn_csv30sec_norm_best_estimator,\n",
    "        knn_csv30sec_std_best_estimator,\n",
    "        knn_csv3sec_best_estimator,\n",
    "        knn_csv3sec_norm_best_estimator,\n",
    "        knn_csv3sec_std_best_estimator\n",
    "    ],\n",
    "    \"Accuracy\": [\n",
    "        knn_csv30sec_accuracy,\n",
    "        knn_csv30sec_norm_accuracy,\n",
    "        knn_csv30sec_std_accuracy,\n",
    "        knn_csv3sec_accuracy,\n",
    "        knn_csv3sec_norm_accuracy,\n",
    "        knn_csv3sec_std_accuracy\n",
    "    ],\n",
    "    \"Run Time\": [\n",
    "        knn_csv30sec_time,\n",
    "        knn_csv30sec_norm_time,\n",
    "        knn_csv30sec_std_time,\n",
    "        knn_csv3sec_time,\n",
    "        knn_csv3sec_norm_time,\n",
    "        knn_csv3sec_std_time\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11292423-e20e-40c5-9613-7e4eb213dfe9",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "Ran two different SVM models, the svm with a linear kernal and tested for the optimal C paramater. We also tested the rbf kernel and looked for the optimal c and gamma\n",
    "\n",
    "Low gamma values mean \"far\" influence, high values mean \"close\" \n",
    "\n",
    "Larger C values may increase fitting time but don't necessarily improve accuracy beyond a certain point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ebc0d8-5604-4271-9680-6a6ae1551d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def testSVM(features, labels, paramgrid):\n",
    "    X_standard_scaler = StandardScaler().fit(features)\n",
    "    features = X_standard_scaler.transform(features)\n",
    "    model = svm.SVC()\n",
    "    best_estimator, accuracy, run_time = girdSearchClassifier(model, features, labels, paramgrid)\n",
    "    \n",
    "    return best_estimator.get_params()['C'], best_estimator.get_params()['gamma'], accuracy, run_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279672f4-a1bc-45f7-9cde-0edd01bdcf05",
   "metadata": {},
   "source": [
    "### Features 3 seconds to large a dataset. Quadratic run time for larger data sets. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f030a65-955f-44df-a1b2-ee1b3a8a26f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_3sec, y_3sec, test_size = 0.01, random_state = 1)\n",
    "sizes = [100, 250, 500, 1000, 2500, 5000, 9000]\n",
    "\n",
    "linear_runtimes = []\n",
    "rbf_runtimes = []\n",
    "\n",
    "X_standard_scaler = StandardScaler().fit(X_train)\n",
    "X_train = X_standard_scaler.transform(X_train)\n",
    "for s in sizes:\n",
    "    start_time = time.time()\n",
    "    classifier = svm.SVC(C = 10, kernel='linear')\n",
    "    classifier.fit(X_train[:s], y_train[:s])\n",
    "    run_time = time.time() - start_time\n",
    "    linear_runtimes.append(run_time)\n",
    "\n",
    "\n",
    "\n",
    "for s in sizes:\n",
    "    start_time = time.time()\n",
    "    classifier = svm.SVC(C = 10, gamma = .1, kernel='rbf')\n",
    "    classifier.fit(X_train[:s], y_train[:s])\n",
    "    run_time = time.time() - start_time\n",
    "    rbf_runtimes.append(run_time)\n",
    "\n",
    "\n",
    "\n",
    "np_linear_runtimes = np.array(linear_runtimes)*5*5*5\n",
    "np_rbf_runtimes = np.array(rbf_runtimes)*5*5*5*5\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(sizes, np_linear_runtimes, label = 'linear SVM')\n",
    "plt.scatter(sizes, np_rbf_runtimes, label = 'rbf SVM')\n",
    "plt.xlabel('training set size')\n",
    "plt.ylabel('time to run grid search')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaec744-4342-47ee-977b-6b6bc1711c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gammas = np.logspace(-5, 2, num=8, endpoint=True, base=10.0)\n",
    "#Cs = np.logspace(-2, 4, num =7, endpoint=True, base=10.0)\n",
    "gammas = [.001, .01, 1, 10, 100]\n",
    "Cs = [1, .01, .1, 1, 10]\n",
    "\n",
    "rbf_svm_param_grid = [\n",
    "  {'C': Cs, \n",
    "   'gamma': gammas, \n",
    "   'kernel': ['rbf']},\n",
    " ]\n",
    "linear_svm_param_grid = [\n",
    "  {'C': Cs, \n",
    "   'kernel': ['linear']},\n",
    " ]\n",
    "\n",
    "print(\"run csv 30sec\")\n",
    "rbf_svm_csv30sec_best_estimator_c, rbf_svm_csv30sec_best_estimator_gamma, rbf_svm_csv30sec_accuracy, rbf_svm_csv30sec_time = testSVM(\n",
    "    X_30sec, y_30sec, rbf_svm_param_grid)\n",
    "\n",
    "print(\"run csv 30sec norm\")\n",
    "rbf_svm_csv30sec_norm_best_estimator_c, rbf_svm_csv30sec_norm_best_estimator_gamma, rbf_svm_csv30sec_norm_accuracy, rbf_svm_csv30sec_norm_time = testSVM(\n",
    "    X_30sec_norm, y_30sec, rbf_svm_param_grid)\n",
    "\n",
    "print(\"run csv 30sec std\")\n",
    "rbf_svm_csv30sec_std_best_estimator_c, rbf_svm_csv30sec_std_best_estimator_gamma, rbf_svm_csv30sec_std_accuracy, rbf_svm_csv30sec_std_time = testSVM(\n",
    "    X_30sec_std, y_30sec, rbf_svm_param_grid)\n",
    "\n",
    "print(\"run csv 30sec linear\")\n",
    "svm_csv30sec_best_estimator_c, svm_csv30sec_best_estimator_gamma, svm_csv30sec_accuracy, svm_csv30sec_time = testSVM(\n",
    "    X_30sec, y_30sec, linear_svm_param_grid)\n",
    "\n",
    "print(\"run csv 30sec norm linear\")\n",
    "svm_csv30sec_norm_best_estimator_c, svm_csv30sec_norm_best_estimator_gamma, svm_csv30sec_norm_accuracy, svm_csv30sec_norm_time = testSVM(\n",
    "    X_30sec_norm, y_30sec, linear_svm_param_grid)\n",
    "\n",
    "print(\"run csv 30sec std linear\")\n",
    "svm_csv30sec_std_best_estimator_c, svm_csv30sec_std_best_estimator_gamma, svm_csv30sec_std_accuracy, svm_csv30sec_std_time = testSVM(\n",
    "    X_30sec_std, y_30sec, linear_svm_param_grid)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Dataset\": [\"Features 30 sec\", \"Features 30 sec norm\", \"Features 30 sec std\", \"Features 30 sec\", \"Features 30 sec norm\", \"Features 30 sec std\"],\n",
    "    \"Kernal\": [\"rbf\", \"rbf\", \"rbf\", \"linear\", \"linear\", \"linear\"],\n",
    "    \"Best C\": [\n",
    "        rbf_svm_csv30sec_best_estimator_c,\n",
    "        rbf_svm_csv30sec_norm_best_estimator_c,\n",
    "        rbf_svm_csv30sec_std_best_estimator_c,\n",
    "        svm_csv30sec_best_estimator_c,\n",
    "        svm_csv30sec_norm_best_estimator_c,\n",
    "        svm_csv30sec_std_best_estimator_c\n",
    "    ],\n",
    "    \"Best gamma\": [\n",
    "        rbf_svm_csv30sec_best_estimator_gamma,\n",
    "        rbf_svm_csv30sec_norm_best_estimator_gamma,\n",
    "        rbf_svm_csv30sec_std_best_estimator_gamma,\n",
    "        'na',  #svm_csv30sec_best_estimator_gamma ,\n",
    "        'na',  #svm_csv30sec_norm_best_estimator_gamma = 'na',\n",
    "        'na'   #svm_csv30sec_std_best_estimator_gamma = 'na'\n",
    "    ],\n",
    "    \"Accuracy\": [\n",
    "        rbf_svm_csv30sec_accuracy,\n",
    "        rbf_svm_csv30sec_norm_accuracy,\n",
    "        rbf_svm_csv30sec_std_accuracy,\n",
    "        svm_csv30sec_accuracy,\n",
    "        svm_csv30sec_norm_accuracy,\n",
    "        svm_csv30sec_std_accuracy\n",
    "    ],\n",
    "    \"Run Time\": [\n",
    "        rbf_svm_csv30sec_time,\n",
    "        rbf_svm_csv30sec_norm_time,\n",
    "        rbf_svm_csv30sec_std_time,\n",
    "        svm_csv30sec_time,\n",
    "        svm_csv30sec_norm_time,\n",
    "        svm_csv30sec_std_time\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6676ab1-6a84-4daf-be20-4626ee12890d",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc764f5-8a60-404f-8083-fce7d3373144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def testRandomForest(features, labels, paramgrid):\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    best_estimator, accuracy, run_time = girdSearchClassifier(model, features, labels, paramgrid)\n",
    "    \n",
    "    return best_estimator, accuracy, run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044dd4f4-9e26-4050-a885-012a6bef14bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"testing\")\n",
    "random_forest_param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "     #'max_features': ['sqrt', 'log2'],\n",
    "     'max_depth': [10, 20, 30, None],\n",
    "     #'min_samples_split': [2, 5, 10],\n",
    "     #'min_samples_leaf': [1, 2, 4],\n",
    "     #'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "random_forest_results = []\n",
    "\n",
    "for X, y, dataset_name_30 in features_datasets_30sec:\n",
    "    print(\"Running \", dataset_name, \"with Random Forest\")\n",
    "    classifier_results(testRandomForest, X, y, dataset_name, random_forest_param_grid, random_forest_results)\n",
    "\n",
    "print(\"First loop done\")\n",
    "for X, y, datset_name_3 in features_datasets_3sec:\n",
    "    print(\"Running \", dataset_name_3, \"with Random Forest\")\n",
    "    classifier_results(testRandomForest, X, y, dataset_name, random_forest_param_grid, random_forest_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eaedfb-0ab7-4f9a-a6fa-b931865fd201",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_forest_results)\n",
    "pd.DataFrame(random_forest_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9947bcd2-18da-4039-a8b9-2242f825150a",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5ea54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TensorFlow setup'''\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f09d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create CNN'''\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(288, 432, 4)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40a7f07e-1daf-486a-8508-eba1b7b0acf7",
   "metadata": {},
   "source": [
    "''' \n",
    "THIS CELL HAS A LONG RUNTIME\n",
    "    By using this command I determined that the model seems to work best when\n",
    "    using kernel size 32 for layer 1, kernel size 16 for layer 2, and no layer 3.\n",
    "    Moving forward I will use this CNN configuration to cross-validate and choose hyperparameters\n",
    "'''\n",
    "def choose_CNN(l1, l2, l3):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(l1, (3, 3), activation='relu', input_shape=(288, 432, 4)))\n",
    "    if l2 != None:\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Conv2D(l2, (3, 3), activation='relu'))\n",
    "    if l3 != None:\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Conv2D(l3, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(10))\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    history = model.fit(X_images_train_tens, y_images_train_tens, epochs=10, validation_data=(X_images_val_tens, y_images_val_tens))\n",
    "    return model\n",
    "\n",
    "'''Train and test CNN'''\n",
    "X_images_train_tens = tf.convert_to_tensor(X_images_train, dtype=float)\n",
    "y_images_train_tens = tf.convert_to_tensor(y_images_train.astype(np.float32), dtype=float)\n",
    "X_images_val_tens = tf.convert_to_tensor(X_images_val, dtype=float)\n",
    "y_images_val_tens = tf.convert_to_tensor(y_images_val.astype(np.float32), dtype=float)\n",
    "\n",
    "layer1 = [16,32,64]\n",
    "layer2 = [None,16,32]\n",
    "layer3 = [None,16,32]\n",
    "allModels = []\n",
    "\n",
    "for l1 in layer1:\n",
    "    for l2 in layer2:\n",
    "        for l3 in layer3:\n",
    "            allModels.append(choose_CNN(l1,l2,l3))\n",
    "\n",
    "for m in allModels:\n",
    "    m.evaluate(X_images_val_tens,  y_images_val_tens, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing hyper for best model (layer 1 = 32, layer 2 = 16)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(288, 432, 4)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "# Set up cross-validation and choose hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44917cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
